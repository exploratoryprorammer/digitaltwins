# -*- coding: utf-8 -*-
"""ucsfresearch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w3PISwyYMwCKuEe9_6pstV9u0Z_3gFGh
"""

# pip install scikit-learn
# pip install -U sentence-transformers
# pip install pinecone
# pip install pandas
# pip install numpy
# pip install matplotlib
# pip install --upgrade google-cloud-bigquery

import os

# Check if running in Colab environment
try:
    from google.colab import auth
    auth.authenticate_user()
    print('Authenticated via Colab')
    project_id = os.environ.get('GOOGLE_CLOUD_PROJECT')
    print(f"Your Colab Project ID is: {project_id}")
except ImportError:
    print('Not running in Colab - using local authentication')
    # For local development, you'll need to set up authentication differently
    # Either set GOOGLE_APPLICATION_CREDENTIALS environment variable
    # or use: gcloud auth application-default login
    project_id = None

import pandas as pd
from google.cloud import bigquery
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
import os

project_id = 'master-anagram-430800-f0'

if not project_id:
    raise ValueError("Google Cloud Project ID not set. Please provide your project ID.")
else:
    print(f"Using Google Cloud Project ID: {project_id}")

client = bigquery.Client(project=project_id)

# Modified queries dictionary with LIMIT 20 added to each query
queries = {
    'age': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.age` LIMIT 20",
    'acei': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.acei` LIMIT 20",
    'invasive_line': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.invasive_line` LIMIT 20",
    'kdigo_creatinine': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.kdigo_creatinine` LIMIT 20",
    'kdigo_stages': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.kdigo_stages` LIMIT 20",
    'kdigo_uo': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.kdigo_uo` LIMIT 20",
    'lods': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.lods` LIMIT 20",
    'meld': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.meld` LIMIT 20",
    'milrinone': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.milrinone` LIMIT 20",
    'neuroblock': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.neuroblock` LIMIT 20",
    'norepinephrine': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.norepinephrine` LIMIT 20",
    'norepinephrine_equivalent_dose': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.norepinephrine_equivalent_dose` LIMIT 20",
    'nsaid': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.nsaid` LIMIT 20",
    'oasis': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.oasis` LIMIT 20",
    'oxygen_delivery': "SELECT * FROM `physionet-data.mimiciv_3_1_derived.oxygen_delivery` LIMIT 20"
}

dataframes = {}

for table_name, query_string in queries.items():
    print(f"\nExecuting query for table: {table_name}")
    try:
        query_job = client.query(query_string)
        df = query_job.to_dataframe()
        dataframes[table_name] = df
        print(f"Successfully loaded data for {table_name} ({len(df)} rows).")
    except Exception as e:
        print(f"Error querying table {table_name}: {e}")
        print(f"Could not load data for {table_name}.")

import pandas as pd
from google.cloud import bigquery
import os

print("\n--- Print the head of each loaded data frame")
if not dataframes:
  print("No tables ind dataframe")
else:
  for table_name, df in dataframes.items():
    print(f"\nHead of {table_name}:")
    print(df.head())

import pandas as pd
from sklearn.preprocessing import StandardScaler
from pinecone import Pinecone, ServerlessSpec
import os
import numpy as np

def create_state_vector_all_features(df_dict):
    state_vectors = {}
    for stay_id in get_all_stay_ids(df_dict):
        stay_data = {}
        age_df = df_dict.get('age')
        if age_df is not None:
            patient_ages = age_df[age_df['subject_id'].isin(get_subject_ids_for_stay(df_dict, stay_id))]
            if not patient_ages.empty:
                stay_data['age'] = patient_ages['age'].iloc[0]
        kdigo_creat_df = df_dict.get('kdigo_creatinine')
        if kdigo_creat_df is not None:
            creat_value = kdigo_creat_df[kdigo_creat_df['stay_id'] == stay_id]['creat'].dropna().iloc[0] if not kdigo_creat_df[kdigo_creat_df['stay_id'] == stay_id]['creat'].dropna().empty else None
            if creat_value is not None:
                stay_data['creat'] = creat_value
        kdigo_stages_df = df_dict.get('kdigo_stages')
        if kdigo_stages_df is not None:
            aki_stage = kdigo_stages_df[kdigo_stages_df['stay_id'] == stay_id]['aki_stage'].dropna().iloc[0] if not kdigo_stages_df[kdigo_stages_df['stay_id'] == stay_id]['aki_stage'].dropna().empty else None
            if aki_stage is not None:
                try:
                    stay_data['aki_stage'] = float(aki_stage)
                except ValueError:
                    stay_data['aki_stage'] = np.nan
        kdigo_uo_df = df_dict.get('kdigo_uo')
        if kdigo_uo_df is not None:
            uo_24hr = kdigo_uo_df[kdigo_uo_df['stay_id'] == stay_id]['uo_rt_24hr'].dropna().iloc[0] if not kdigo_uo_df[kdigo_uo_df['stay_id'] == stay_id]['uo_rt_24hr'].dropna().empty else None
            if uo_24hr is not None:
                try:
                    stay_data['uo_rt_24hr'] = float(uo_24hr)
                except ValueError:
                    stay_data['uo_rt_24hr'] = np.nan
        lods_df = df_dict.get('lods')
        if lods_df is not None:
            lods_score = lods_df[lods_df['stay_id'] == stay_id]['lods'].dropna().iloc[0] if not lods_df[lods_df['stay_id'] == stay_id]['lods'].dropna().empty else None
            if lods_score is not None:
                try:
                    stay_data['lods'] = float(lods_score)
                except ValueError:
                    stay_data['lods'] = np.nan
            for col in ['neurologic', 'cardiovascular', 'renal', 'pulmonary', 'hematologic', 'hepatic']:
                value = lods_df[lods_df['stay_id'] == stay_id][col].dropna().iloc[0] if not lods_df[lods_df['stay_id'] == stay_id][col].dropna().empty else None
                if value is not None:
                    try:
                        stay_data[f'lods_{col}'] = float(value)
                    except ValueError:
                        stay_data[f'lods_{col}'] = np.nan
        oasis_df = df_dict.get('oasis')
        if oasis_df is not None:
            oasis_score = oasis_df[oasis_df['stay_id'] == stay_id]['oasis'].dropna().iloc[0] if not oasis_df[oasis_df['stay_id'] == stay_id]['oasis'].dropna().empty else None
            if oasis_score is not None:
                try:
                    stay_data['oasis'] = float(oasis_score)
                except ValueError:
                    stay_data['oasis'] = np.nan
            for col in ['gcs', 'resprate', 'temp', 'urineoutput']:
                value = oasis_df[oasis_df['stay_id'] == stay_id][col].dropna().iloc[0] if not oasis_df[oasis_df['stay_id'] == stay_id][col].dropna().empty else None
                if value is not None:
                    try:
                        stay_data[f'oasis_{col}'] = float(value)
                    except ValueError:
                        stay_data[f'oasis_{col}'] = np.nan
        if stay_data:
            state_vectors[stay_id] = stay_data
    return pd.DataFrame.from_dict(state_vectors, orient='index')

def get_all_stay_ids(df_dict):
    stay_ids = set()
    for df in df_dict.values():
        if 'stay_id' in df.columns:
            stay_ids.update(df['stay_id'].unique())
    return list(stay_ids)

def get_subject_ids_for_stay(df_dict, stay_id):
    subject_ids = set()
    for df in df_dict.values():
        if 'stay_id' in df.columns and 'subject_id' in df.columns:
            subject_ids.update(df[df['stay_id'] == stay_id]['subject_id'].unique())
        elif 'subject_id' in df.columns and 'hadm_id' in df.columns:
            pass
    for df in df_dict.values():
        if 'stay_id' in df.columns and 'subject_id' in df.columns and not df[df['stay_id'] == stay_id]['subject_id'].empty:
            return df[df['stay_id'] == stay_id]['subject_id'].unique()
        elif 'hadm_id' in df.columns and 'subject_id' in df.columns:
            pass
    return set()

patient_states_all_df = create_state_vector_all_features(dataframes)

if not patient_states_all_df.empty:
    patient_states_imputed_df = patient_states_all_df.fillna(patient_states_all_df.mean(numeric_only=True)).fillna(0)
    scaler_all = StandardScaler()
    scaled_states_all = scaler_all.fit_transform(patient_states_imputed_df)
    scaled_states_all_df = pd.DataFrame(scaled_states_all, index=patient_states_imputed_df.index, columns=patient_states_imputed_df.columns)

    vectors_for_pinecone = []
    for stay_id, row in scaled_states_all_df.iterrows():
        vector = row.tolist()
        print(f"Stay ID: {stay_id}, Vector (before adding to upsert): {vector}") # ADDED PRINT STATEMENT
        vectors_for_pinecone.append((str(stay_id), vector))

    print("\nScaled state vectors prepared for Pinecone (first 5):")
    for i in range(min(5, len(vectors_for_pinecone))):
        print(f"ID: {vectors_for_pinecone[i][0]}, Vector: {vectors_for_pinecone[i][1]}")

else:
    print("\nNo patient state vectors could be created.")





from pinecone import Pinecone

pc = Pinecone(api_key="08180080-18aa-4999-b131-612b5b9b41aa")
index_name = "ucsfresearch"

try:
    if pc.has_index(index_name):
        pc.delete_index(index_name)
        print(f"Deleted Pinecone index '{index_name}'.")
    else:
        print(f"Index '{index_name}' does not exist.")
except Exception as e:
    print(f"Error deleting index: {e}")

from pinecone import Pinecone, ServerlessSpec
import os
import numpy as np

def safe_meta(value, fallback):
    return fallback if value is None else value

if 'scaled_states_all_df' in locals() and not scaled_states_all_df.empty:
    pc = Pinecone(api_key="08180080-18aa-4999-b131-612b5b9b41aa")
    index_name = "ucsfresearch"
    vector_dimension = scaled_states_all_df.shape[1]

    try:
        if not pc.has_index(index_name):
            print(f"Creating index '{index_name}' with dimension {vector_dimension}...")
            pc.create_index(
                name=index_name,
                dimension=vector_dimension,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1")
            )
            print(f"Index '{index_name}' created.")
        else:
            print(f"Index '{index_name}' exists.")
            existing_index_info = pc.describe_index(index_name)
            if existing_index_info.dimension != vector_dimension:
                print(f"WARNING: Index dimension ({existing_index_info.dimension}) does not match vector dimension ({vector_dimension}).")

        index = pc.Index(index_name)

        vectors_to_upsert = []
        epsilon = 1e-9  # Threshold for non-zero vectors

        for stay_id, row in scaled_states_all_df.iterrows():
            vector = np.array(row.tolist())

            if np.any(np.abs(vector) > epsilon):
                # Replace with actual sources if these columns are from another DataFrame
                subject_id = row.get("subject_id", "unknown")
                treatment = row.get("treatment", "unknown")
                mortality = row.get("mortality_30d", False)

                vectors_to_upsert.append({
                    "id": str(stay_id),
                    "values": vector.tolist(),
                    "metadata": {
                        "subject_id": str(safe_meta(subject_id, "unknown")),
                        "treatment": str(safe_meta(treatment, "unknown")),
                        "mortality_30d": bool(safe_meta(mortality, False))
                    }
                })
            else:
                print(f"Skipping all-zero vector with ID: {stay_id} (values effectively zero)")

        if vectors_to_upsert:
            print(f"Upserting {len(vectors_to_upsert)} valid vectors...")
            index.upsert(
                vectors=vectors_to_upsert,
                namespace="data"
            )
            print(f"Upserted {len(vectors_to_upsert)} vectors.")
            print("\nIndex Stats:")
            print(index.describe_index_stats())
        else:
            print("No valid vectors to upsert (all were effectively all-zeros).")

    except Exception as e:
        print(f"\nPinecone error: {e}")
else:
    print("No scaled state vectors to upsert.")

from pinecone import Pinecone
import numpy as np

pc = Pinecone(api_key="08180080-18aa-4999-b131-612b5b9b41aa")
index_name = "ucsfresearch"

def find_similar_patients(patient_state_vector, top_k=5):
    if len(patient_state_vector) != 14:
        print(f"Error: Input vector must have 14 dimensions, but got {len(patient_state_vector)}.")
        return None
    try:
        index = pc.Index(index_name)
        query_results = index.query(
            vector=patient_state_vector,
            namespace="data",
            top_k=top_k,
            include_metadata=True
        )
        return query_results.matches
    except Exception as e:
        print(f"Error querying Pinecone: {e}")
        return None

if 'scaled_states_all_df' in locals() and not scaled_states_all_df.empty:
    example_patient_vector = scaled_states_all_df.iloc[0].tolist()
    print("Example input patient state vector:", example_patient_vector)

    similar_patients = find_similar_patients(example_patient_vector, top_k=3)

    if similar_patients:
        print("\nTop 3 most similar patients:")
        for match in similar_patients:
            print(f"ID: {match.id}, Similarity Score: {match.score}")
    else:
        print("Could not retrieve similar patients.")
else:
    print("No scaled data available to create an example input vector.")

print("\nIndex Stats:")
print(index.describe_index_stats())



# Making a Decision Tree(Random Forest)
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
import graphviz